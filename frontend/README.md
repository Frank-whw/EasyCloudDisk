# EasyCloudDisk - 前端文档


## 🌟 功能特性

目前前端已集成并完善以下核心功能：

### 1. 🔐 用户认证
*   **注册**: 支持邮箱注册，前端校验密码强度（至少8位，含字母数字）。
*   **登录**: 使用 JWT Token 认证，支持自动保存登录状态。
*   **安全**: 自动处理 Token 过期（401）跳转，多用户数据完全隔离。

### 2. 📁 基础文件管理
*   **文件列表**: 
    *   支持 **网格视图** 和 **列表视图** 切换。
    *   支持 **面包屑导航**，可逐级进入/返回目录。
    *   支持 **文件名搜索**（实时过滤）。
*   **文件操作**:
    *   **上传**: 支持点击按钮选择或**拖拽上传**。会自动上传到当前所在目录。
    *   **下载**: 支持二进制流式下载（智能识别文件/文件夹，文件夹仅支持删除）。
    *   **删除**: 二次确认删除，实时刷新列表。
*   **目录管理**:
    *   支持新建文件夹。
    *   点击文件夹图标可进入子目录。

### 3. 🚀 高级传输功能

#### 3.1 网络流量优化技术

##### 3.1.1 数据压缩 ⚙️
*   **服务端压缩**: 上传时对每个块进行GZIP压缩（如果压缩后更小）
    *   后端自动处理，前端无需实现
    *   压缩后的数据存储到S3，元数据标记压缩状态
*   **自动解压**: 下载时根据元数据自动解压缩
    *   后端自动识别压缩状态并解压
    *   前端下载时自动获得解压后的原始文件
*   **存储优化**: S3存储压缩后的数据，元数据标记压缩状态

##### 3.1.2 文件级去重（秒传功能）✅
*   **计算文件哈希值**: 前端使用 Web Crypto API 计算 SHA-256 哈希
    *   实现位置: `hashCalculator.js` → `calculateFullFileHash()`
*   **上传前检查**: 调用 API `POST /files/quick-check` 检查哈希是否存在
    *   前端实现: `uploadManager.js` → `uploadWithQuickCheck()`
    *   后端处理: `AdvancedUploadService.checkQuickUpload()`
*   **秒传机制**: 如果服务器已有相同哈希的文件，直接创建指针而非重复存储
    *   前端调用: `POST /files/quick-upload`
    *   后端处理: `AdvancedUploadService.quickUpload()`
    *   **无需传输数据**，直接复制引用，瞬间完成上传

##### 3.1.3 块级去重 ⚙️
*   **块切分**: 固定4MB块大小切分大文件（后端自动处理）
    *   后端实现: `ChunkService.storeFileInChunks()` → `splitIntoChunks()`
*   **哈希计算**: 每个块计算SHA-256哈希（后端自动处理）
    *   后端实现: `ChunkService.storeFileInChunks()` → `DigestUtils.sha256Hex()`
*   **去重存储**: 仅上传服务器不存在的新块
    *   后端自动检查: `chunkRepository.findByChunkHash()`
    *   已存在的块会被复用，引用计数+1
*   **引用计数**: 自动管理块引用，删除时智能清理
    *   删除文件时: `ChunkService.deleteFileChunks()` 自动减少引用计数
    *   引用计数为0时自动删除块
*   **元数据组装**: 通过`file_chunk_mappings`表记录文件与块的关系
    *   后端自动维护映射关系

**注意**: 
- ✅ **已完整实现并自动调用**：所有文件上传时都自动启用块级去重和数据压缩
  - **小文件（<2MB）**：直接上传，自动启用块级去重和压缩
  - **大文件（>2MB）**：断点续传，合并后自动启用块级去重和压缩
- ✅ **秒传功能**：所有文件（无论大小）上传前都会自动检查秒传

#### 3.2 断点续传
*   大文件（>2MB）自动启用分块上传。
*   支持上传进度条实时显示。
*   前端实现: `uploadManager.js` → `uploadWithResumable()`

#### 3.3 实时同步
*   基于 SSE (Server-Sent Events) 技术。
*   当文件在其他客户端（如 Python 同步脚本）发生变动时，网页端列表会**自动刷新**并弹出通知。
*   前端实现: `syncManager.js`

---

## 🧪 测试方法指南

### 准备工作
**1、确保后端服务已启动 (`localhost:8080`)。**

补充启动后端服务的方法：

**启动后端**（终端1）

```bash
# 在WSL中
cd server
export AWS_ACCESS_KEY_ID="AKIARCSPQ2MSDC2UES4A"
export AWS_SECRET_ACCESS_KEY="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"
export AWS_REGION="ap-northeast-1"
export AWS_S3_BUCKET="clouddisk-test-1762861672"

# 启动服务
mvn spring-boot:run

# 或者使用启动脚本
chmod +x start.sh
./dev-start.sh
```

**2、使用 Python 启动前端服务（避免 CORS 问题）：**

```bash
cd frontend
python -m http.server 3000
```

**3、浏览器访问: `http://localhost:3000`**



### 场景一：基础功能流程

1. **注册/登录**:
   * 注册一个新账号（如 `test@demo.com`）。
   * 登录后进入主界面。
2. **文件夹操作**:
   * 点击工具栏“新建文件夹”，输入 `我的文档`。
   * 点击生成的文件夹图标，进入该目录。
   * 确认面包屑导航显示：`根目录 > 我的文档`。
3. **文件上传**:
   * 拖拽一个 `test.txt` 到网页中间区域。
   * 确认文件出现在当前目录下（非根目录）。
4. **文件下载/删除**:
   * 双击文件或点击“下载”按钮，验证文件内容。
   * 点击“删除”按钮，验证文件消失。

### 场景二：文件级去重（秒传）测试

#### 测试步骤

1. **准备测试文件**:
   * 创建一个文本文件 `test_upload.txt`，内容如下：
     ```
     这是一个测试文件，用于测试秒传功能。
     内容可以任意填写，用于验证文件哈希计算。
     ```
   * 保存文件（确保内容唯一，便于测试）

2. **首次上传**:
   * 在网页中点击“上传”按钮或拖拽 `test_upload.txt` 到网页中间区域
   * **观察现象**:
     - 提示信息显示“正在计算文件哈希: test_upload.txt...”
     - 进度条从 0% → 10% → 100% 正常走完上传过程
     - 上传完成后提示“上传成功: test_upload.txt”
   * 确认文件出现在文件列表中

3. **删除文件（测试秒传前提）**:
   * 在文件列表中点击 `test_upload.txt` 的“删除”按钮
   * 确认删除成功（注意：文件块仍在服务器S3中，仅删除了文件引用）

4. **再次上传相同文件（验证秒传）**:
   * **重要**：再次上传**完全相同**的 `test_upload.txt`（确保文件内容、大小完全一致）
   * **预期现象**:
     - 提示信息显示“正在计算文件哈希: test_upload.txt...”
     - 进度条**直接从50%跳到100%**（而不是从0%开始）
     - 提示信息显示**“秒传成功: test_upload.txt”**（而不是“上传成功”）
     - 上传完成速度**明显快于首次上传**（几乎瞬间完成）
   * **验证点**:
     - ✅ 显示“秒传成功”提示
     - ✅ 上传速度极快（无需实际传输数据）
     - ✅ 文件正常出现在列表中
     - ✅ 下载后文件内容与原始文件完全一致

#### 测试失败排查
* 如果未触发秒传，检查：
  1. 文件内容是否完全相同（不能有任何修改）
  2. 前端控制台是否显示哈希计算过程
  3. 后端日志是否显示调用了 `/files/quick-check` API
  4. 文件大小是否小于2MB（大文件使用断点续传，目前未实现秒传检查）

### 场景三：块级去重测试

#### 测试步骤

1. **准备大文件**:
   * 创建一个大于4MB的测试文件 `large_file.bin`（约10-15MB）
   * **Windows PowerShell 创建方法**:
     ```powershell
     # 创建10MB文件
     [System.IO.File]::WriteAllBytes("large_file.bin", (New-Object byte[] (10*1024*1024)))
     ```
   * 或使用文本编辑器创建一个包含大量重复内容的文本文件

2. **首次上传大文件**:
   * 上传 `large_file.bin`（文件大小 >2MB，会触发断点续传）
   * **前端观察现象**:
     - 提示信息："开始分块上传: large_file.bin (共X个分块，每块2MB)"
     - 进度条显示分块上传过程，状态显示："上传中... (X/Y 分块)"
     - 上传完成后提示："上传成功: large_file.bin (已进行块级去重和压缩，详情请查看后端日志)"
     - 文件出现在列表中
   * **后端日志观察**（查看后端控制台）:
     - 查找类似日志：
       ```
       创建断点续传会话: sessionId=..., fileName=large_file.bin, totalChunks=5
       上传分块成功: sessionId=..., chunkIndex=0, progress=1/5
       上传分块成功: sessionId=..., chunkIndex=1, progress=2/5
       ...
       断点续传完成: sessionId=..., fileName=large_file.bin, fileId=..., 上传分块数=5, 文件大小=10.00MB
       开始块级去重处理: fileId=..., 文件将被切分为4MB块进行去重和压缩
       文件分块处理: fileId=..., 总块数=3, 文件大小=10.00MB
       新块上传: fileId=..., 块索引=1/3, 块大小=4096.00KB, 哈希=abc12345...
       新块上传: fileId=..., 块索引=2/3, 块大小=4096.00KB, 哈希=def67890...
       新块上传: fileId=..., 块索引=3/3, 块大小=1808.00KB, 哈希=ghi11111...
       块级去重完成: fileId=..., 总块数=3, 新块=3, 复用块=0, 去重率=0.0%
       ```
   * **关键信息**:
     - `总块数=3`：文件被切分为3个4MB块
     - `新块=3, 复用块=0`：所有块都是新的，没有复用
     - `去重率=0.0%`：首次上传，没有块被复用

3. **修改文件后再次上传（验证块级去重）**:
   * 对 `large_file.bin` 做**小修改**（例如改变文件开头几个字节，只影响第一个4MB块）
   * 重命名为 `large_file_v2.bin` 并上传
   * **前端观察现象**:
     - 分块上传过程正常进行
     - 上传完成后提示："上传成功: large_file_v2.bin (已进行块级去重和压缩，详情请查看后端日志)"
   * **后端日志观察**（关键验证点）:
     - 查找类似日志：
       ```
       文件分块处理: fileId=..., 总块数=3, 文件大小=10.00MB
       新块上传: fileId=..., 块索引=1/3, 块大小=4096.00KB, 哈希=xyz99999...  (第一个块被修改，是新块)
       块复用: fileId=..., 块索引=2/3, 哈希=def67890..., 引用计数=2  (第二个块复用)
       块复用: fileId=..., 块索引=3/3, 哈希=ghi11111..., 引用计数=2  (第三个块复用)
       块级去重完成: fileId=..., 总块数=3, 新块=1, 复用块=2, 去重率=66.7%
       ```
   * **验证点**:
     - ✅ `新块=1, 复用块=2`：只有第一个块是新上传的，其他2个块被复用
     - ✅ `去重率=66.7%`：66.7%的块被复用，节省了存储空间和上传时间
     - ✅ `引用计数=2`：被复用的块引用计数增加（说明多个文件共享同一个块）
     - ✅ 上传速度**快于上传全新文件**（因为只有1个新块需要上传到S3）

4. **上传完全相同文件（验证块级去重）**:
   * 再次上传原始 `large_file.bin`（确保内容完全相同）
   * **前端观察现象**:
     - 如果文件哈希相同，会触发**秒传**（显示"秒传成功"）
     - 如果文件名不同，会进行正常上传流程
   * **后端日志观察**（如果未触发秒传）:
     - 查找类似日志：
       ```
       文件分块处理: fileId=..., 总块数=3, 文件大小=10.00MB
       块复用: fileId=..., 块索引=1/3, 哈希=abc12345..., 引用计数=2  (所有块都复用)
       块复用: fileId=..., 块索引=2/3, 哈希=def67890..., 引用计数=3
       块复用: fileId=..., 块索引=3/3, 哈希=ghi11111..., 引用计数=2
       块级去重完成: fileId=..., 总块数=3, 新块=0, 复用块=3, 去重率=100.0%
       ```
   * **验证点**:
     - ✅ `新块=0, 复用块=3`：所有块都被复用，没有新块上传
     - ✅ `去重率=100.0%`：100%的块被复用，完全避免了重复存储
     - ✅ `引用计数`增加：说明多个文件共享这些块
     - ✅ 上传速度**非常快**（所有块都无需上传到S3，只需创建文件-块映射关系）

#### 验证点
* ✅ 大文件被正确切分为4MB块（后端日志显示"总块数=X"）
* ✅ 相同内容的块被复用，不会重复存储（日志显示"块复用"和"引用计数"）
* ✅ 去重率正确计算（日志显示"去重率=X%"）
* ✅ 文件删除时，块的引用计数正确减少（查看后端日志或数据库）
* ✅ 引用计数为0的块被自动删除（后端日志显示"Deleting chunk"）

#### 如何查看后端日志

**方法1：查看后端控制台输出**
- 启动后端服务后，所有日志会输出到控制台
- 关键日志级别为 `INFO`，会显示分块和去重信息

**方法2：查看日志文件**（如果配置了日志文件）
- 日志文件通常位于 `server/logs/` 目录
- 使用文本编辑器或 `tail -f` 命令实时查看

**方法3：使用数据库查询**（高级）
- 查询 `file_chunks` 表查看所有块的信息
- 查询 `file_chunk_mappings` 表查看文件与块的映射关系
- 查看 `ref_count` 字段了解块的引用计数

#### 关键日志说明

| 日志信息 | 含义 |
|---------|------|
| `文件分块处理: 总块数=X` | 文件被切分为X个4MB块 |
| `新块上传: 块索引=X/Y` | 第X个块是新块，需要上传到S3 |
| `块复用: 块索引=X/Y, 引用计数=N` | 第X个块已存在，被复用，引用计数为N |
| `块级去重完成: 新块=X, 复用块=Y, 去重率=Z%` | 去重统计：X个新块，Y个复用块，去重率Z% |

#### 注意事项
* ✅ **所有文件**（无论大小）上传时，块级去重功能**已完整实现并自动调用**
* ✅ 大文件（>2MB）先进行断点续传（2MB分块），完成后进行块级去重（4MB分块）
* ✅ 小文件（<2MB）直接进行块级去重（如果文件大于4MB会被切分，否则作为单个块处理）

---

### 场景四：数据压缩测试

#### 测试步骤

**注意**：数据压缩功能在后端自动执行，前端无感知，但可以通过以下方式间接验证。

1. **准备可压缩文件**:
   * 创建一个包含大量重复内容的文本文件 `compressible.txt`
   * **创建方法**（PowerShell）:
     ```powershell
     # 创建包含重复内容的文件
     $content = "重复内容重复内容重复内容..." * 1000
     [System.IO.File]::WriteAllText("compressible.txt", $content)
     ```
   * 或手动复制粘贴重复文本，使文件达到几百KB或几MB

2. **上传文件**:
   * 上传 `compressible.txt`
   * **观察前端**：正常上传流程，无特殊提示

3. **验证压缩效果（查看后端日志）**:
   * 查看后端控制台日志，查找类似信息：
     ```
     Uploading new chunk: hash=..., size=..., compress=true
     Compression not beneficial, using original data  (如果压缩后更大)
     或
     Chunk uploaded successfully: storageKey=...       (压缩后存储)
     ```
   * **验证压缩状态**：
     - 检查数据库 `file_chunks` 表的 `compressed` 字段
     - 压缩后的块 `compressed=true`
     - 未压缩的块 `compressed=false`

4. **下载文件验证自动解压**:
   * 点击下载 `compressible.txt`
   * **预期现象**:
     - 下载的文件内容与原始文件**完全一致**
     - 文件大小与原始文件相同（已自动解压）
     - 文件内容完整无误

#### 验证点
* ✅ 可压缩文件（如文本文件）被自动压缩后存储
* ✅ 已压缩的文件下载时自动解压
* ✅ 压缩后更大的文件使用原始数据（压缩标志为false）
* ✅ S3存储的是压缩后的数据（节省存储空间）

#### 压缩规则
* 后端自动判断：如果压缩后更小，则存储压缩数据；否则存储原始数据
* 压缩状态记录在 `file_chunks.compressed` 字段中
* 下载时根据 `compressed` 标志自动决定是否解压

---

### 场景四：差分同步（增量同步）测试

#### 测试步骤

1. **准备测试文件**:
   * 创建一个大于4MB的文本文件 `delta_test.txt`（约10MB）
   * **Windows PowerShell 创建方法**:
     ```powershell
     # 创建10MB文件（包含重复内容）
     $content = "测试内容 " * 1000000
     [System.IO.File]::WriteAllText("delta_test.txt", $content)
     ```

2. **首次上传文件**:
   * 在网页中上传 `delta_test.txt`
   * 确认文件上传成功并出现在文件列表中
   * 记录文件ID（可通过浏览器开发者工具查看）

3. **修改文件（模拟本地编辑）**:
   * 在本地打开 `delta_test.txt`
   * 修改文件开头部分（例如添加几行新内容），只影响第一个4MB块
   * 保存文件

4. **执行差分同步**:
   * 在文件列表中，找到 `delta_test.txt` 文件
   * 点击文件操作中的 **"差分同步"** 按钮
   * 在弹出的对话框中选择修改后的 `delta_test.txt` 文件
   * 点击"开始同步"

5. **观察现象**:
   * **前端观察**:
     - 提示信息："开始差分同步: delta_test.txt..."
     - 系统自动获取服务器文件签名
     - 比对差异，识别变更块
     - 提示："差分同步成功: delta_test.txt (更新了 X 个块)"
   * **后端日志观察**（查看后端控制台）:
     - 查找类似日志：
       ```
       获取文件签名: fileId=..., 块数=3
       应用差异块: fileId=..., chunkIndex=0, size=4194304
       差分同步完成: fileId=..., newVersion=2, deltaCount=1, totalSize=10.00MB
       ```
   * **验证点**:
     - ✅ 只上传了变更的块（第一个块），其他块被复用
     - ✅ 文件版本号增加（version从1变为2）
     - ✅ 上传速度明显快于完整上传（只传输了变更部分）
     - ✅ 下载后文件内容与修改后的文件完全一致

6. **多次修改测试**:
   * 再次修改文件（例如修改中间部分，影响第二个块）
   * 执行差分同步
   * **验证点**:
     - ✅ 只有第二个块被更新
     - ✅ 第一个和第三个块被复用
     - ✅ 版本号继续递增

#### 验证点
* ✅ 差分同步功能正常工作
* ✅ 只上传变更的块，节省带宽
* ✅ 文件版本号正确递增
* ✅ 文件内容完整性验证通过

#### 注意事项
* 差分同步适用于**大文件频繁小改动**的场景（如文档编辑、日志追加）
* 文件必须已经通过块级去重存储（storageKey='chunked'）
* 块大小为4MB，只有变更的块会被上传

---

### 场景五：数据加密上传测试

#### 测试步骤

1. **准备测试文件**:
   * 创建一个包含敏感信息的文本文件 `secret.txt`
   * 内容示例：
     ```
     这是敏感信息
     密码: 123456
     账号: admin
     ```

2. **执行加密上传**:
   * 在网页中点击侧边栏的 **"加密上传"** 按钮
   * 在弹出的对话框中：
     - 选择 `secret.txt` 文件
     - 输入加密密码（例如：`mySecretPassword123`）
     - 选择加密算法（推荐：AES-256-GCM）
     - 可选择是否使用收敛加密（支持去重但安全性较低）
   * 点击"上传"

3. **观察现象**:
   * **前端观察**:
     - 提示信息："正在加密文件: secret.txt..."
     - 进度条显示加密和上传过程
     - 提示："加密文件上传成功: secret.txt"
     - 文件出现在列表中（可能显示为加密文件图标）
   * **后端日志观察**（查看后端控制台）:
     - 查找类似日志：
       ```
       上传加密文件: fileName=secret.txt, algorithm=AES-256-GCM
       保存加密元数据: fileId=..., algorithm=AES-256-GCM, salt=..., iv=...
       ```
   * **验证点**:
     - ✅ 文件上传成功
     - ✅ 加密元数据已保存

4. **验证加密效果**:
   * 查看数据库 `file_encryption_metadata` 表
   * 确认加密元数据已保存：
     - `algorithm`: AES-256-GCM 或 AES-256-CBC
     - `salt`: 盐值（Base64编码）
     - `iv`: 初始化向量（Base64编码）
     - `iterations`: 密钥派生迭代次数
     - `convergent`: 是否使用收敛加密
     - `original_hash`: 原始文件哈希（用于完整性校验）

5. **收敛加密测试（可选）**:
   * 使用相同的文件和密码，选择"使用收敛加密"
   * 上传文件
   * **验证点**:
     - ✅ 收敛加密文件支持去重（相同明文+相同密码=相同密文）
     - ✅ 如果服务器已有相同收敛加密文件，可以秒传

#### 验证点
* ✅ 文件在客户端被正确加密
* ✅ 加密元数据正确保存到服务器
* ✅ 服务器存储的是加密后的数据（无法直接读取原始内容）
* ✅ 收敛加密支持去重功能

#### 注意事项
* **标准加密（AES-256-GCM）**：安全性高，但无法去重
* **收敛加密**：支持去重，但安全性较低（相同明文+相同密码=相同密文）
* 加密密码由用户保管，服务器无法解密文件内容
* 下载加密文件后，需要客户端使用相同密码解密

#### 加密算法说明
* **AES-256-GCM**（推荐）：提供加密和完整性校验，安全性高
* **AES-256-CBC**：传统加密算法，需要额外的完整性校验
* **密钥派生**：使用PBKDF2算法，默认迭代次数100000次

---

### 场景六：跨客户端同步测试
*需配合 Python 客户端脚本 (`sync_client/sync_client.py`)*

1. **启动 Python 客户端**:
   * 修改脚本中的账号为当前登录账号。
   * 运行脚本：`python sync_client.py`

2. **本地 -> 云端**:
   * 在本地 `sync_folder` 中新建文件 `sync_test.txt`。
   * **观察网页端**：应自动弹出“文件已同步”提示，并自动刷新出新文件。

3. **云端 -> 本地 (通知)**:
   * 在网页端上传 `web_upload.txt`。
   * **观察 Python 控制台**：应打印“收到服务器通知”。

---

---

## 📋 功能集成状态总结

### ✅ 已完整集成（前端自动调用后端）

| 功能 | 前端实现 | 后端实现 | 自动调用 | 测试状态 |
|------|---------|---------|---------|---------|
| **文件级去重（秒传）** | ✅ `hashCalculator.js`<br>`uploadManager.js` | ✅ `AdvancedUploadService` | ✅ **所有文件自动调用** | ✅ 已测试 |
| **块级去重** | ❌ 前端不实现 | ✅ `ChunkService` | ✅ **所有文件自动调用** | ✅ 已测试 |
| **数据压缩** | ❌ 前端不实现 | ✅ `ChunkService.uploadNewChunk()` | ✅ **所有文件自动调用** | ✅ 已测试 |
| **自动解压** | ❌ 前端不实现 | ✅ `S3StorageService.loadFile()` | ✅ 下载时自动调用 | ✅ 已测试 |
| **断点续传** | ✅ `uploadManager.js` | ✅ `AdvancedUploadService` | ✅ 大文件自动调用 | ✅ 已测试 |
| **差分同步** | ✅ `deltaSyncManager.js` | ✅ `DiffSyncService` | ⚠️ 手动调用 | ✅ 已测试 |
| **数据加密** | ✅ `encryptionManager.js` | ✅ `EncryptionService` | ⚠️ 手动调用 | ✅ 已测试 |

**所有功能已完整集成！** ✅

- ✅ **小文件（<2MB）**：自动使用秒传检查 + 块级去重 + 数据压缩
- ✅ **大文件（>2MB）**：自动使用秒传检查 + 断点续传 + 块级去重 + 数据压缩

### 📝 技术说明

**前端与后端关系**：
- 前端（JavaScript）通过 **HTTP API** 调用后端（Java Spring Boot）
- 前端负责：用户界面、文件读取、哈希计算、文件切分（断点续传）、API调用
- 后端负责：块级去重、数据压缩、文件存储（S3）、数据库操作、所有业务逻辑
- **所有核心优化功能（压缩、去重）都由后端自动处理**，前端只需调用API即可

**API调用链**：

**小文件上传（<2MB）**：
```
前端 uploadWithQuickCheck() 
  → POST /files/quick-check (检查秒传)
  → POST /files/quick-upload (秒传) 或
  → POST /files/upload (普通上传)
    → FileService.upload()
      → ChunkService.storeFileInChunks()
        → 块级去重 + 数据压缩（自动执行）
```

**大文件上传（>2MB）**：
```
前端 uploadWithResumable() 
  → 计算文件哈希
  → POST /files/quick-check (检查秒传)
  → POST /files/quick-upload (秒传) 或
  → POST /files/resumable/init (初始化断点续传)
  → POST /files/resumable/{id}/chunk/{index} (上传分块，循环)
  → POST /files/resumable/{id}/complete (完成上传)
    → AdvancedUploadService.completeResumableUpload()
      → 合并分块
      → ChunkService.storeFileInChunks()
        → 块级去重 + 数据压缩（自动执行）
```

---

## 4. 🔒 高级优化技术

### 4.1 差分同步（增量同步）✅

差分同步功能允许用户仅上传文件的变更部分，而不是整个文件，特别适用于大文件频繁小改动的场景。

#### 功能特性
*   **块签名获取**: `GET /files/{fileId}/signatures` 获取文件所有块的哈希
*   **差异检测**: 客户端比对本地和服务器块签名，识别变更块
*   **增量上传**: `POST /files/{fileId}/delta` 仅上传变更的块
*   **服务器合并**: 复用未变更块，应用变更块，生成新版本
*   **适用场景**: 大文件频繁小改动（如文档编辑、日志追加）

#### 使用方法
1. 在文件列表中，找到需要更新的文件
2. 点击文件操作中的 **"差分同步"** 按钮
3. 选择本地已修改的文件
4. 系统自动检测差异并仅上传变更的块

#### 技术实现
*   **前端实现**: `deltaSyncManager.js`
    *   获取服务器文件签名
    *   计算本地文件块哈希
    *   比对差异，识别变更块
    *   将变更块转换为Base64编码并上传
*   **后端实现**: `DiffSyncService`
    *   解析Base64编码的变更块
    *   复用未变更的块
    *   应用变更块，生成新版本
    *   自动进行块级去重和压缩

### 4.2 断点续传 ✅

断点续传功能已完整实现，详见"场景二：断点续传测试"部分。

#### 功能特性
*   **会话管理**: `POST /files/resumable/init` 创建上传会话
*   **分块上传**: `POST /files/resumable/{sessionId}/chunk/{index}` 上传单个2MB分块
*   **进度跟踪**: `GET /files/resumable/sessions` 查看所有上传会话及进度
*   **断点恢复**: 中断后可继续上传未完成的分块
*   **自动清理**: 24小时后自动清理过期会话
*   **完成合并**: `POST /files/resumable/{sessionId}/complete` 合并所有分块

### 4.3 数据加密 ✅

数据加密功能支持客户端加密上传，确保文件在传输和存储过程中的安全性。

#### 功能特性
*   **客户端加密支持**: 服务端存储加密元数据
*   **加密算法**: 支持 AES-256-GCM、AES-256-CBC
*   **密钥派生**: 支持 PBKDF2（默认迭代次数100000次）
*   **元数据管理**: 存储加密算法、盐值、IV、迭代次数
*   **收敛加密**: 支持收敛加密模式（可去重但安全性较低）
*   **完整性校验**: 存储原始文件哈希用于验证

#### 使用方法
1. 点击侧边栏的 **"加密上传"** 按钮
2. 选择要加密的文件
3. 输入加密密码
4. 选择加密算法（推荐：AES-256-GCM）
5. 可选择是否使用收敛加密（支持去重但安全性较低）
6. 点击"上传"

#### 技术实现
*   **前端实现**: `encryptionManager.js`
    *   使用 Web Crypto API 进行客户端加密
    *   支持 AES-256-GCM 和 AES-256-CBC 算法
    *   使用 PBKDF2 进行密钥派生
    *   生成随机盐值和IV（收敛加密使用固定盐值）
    *   计算原始文件哈希用于完整性校验
*   **后端实现**: `EncryptionService`
    *   存储加密元数据到数据库
    *   支持收敛加密秒传检查
    *   文件以加密形式存储到S3

#### 加密方案说明
*   **方案1（标准加密）**: 客户端加密，服务端存储密文，**无法去重**，安全性高
*   **方案2（收敛加密）**: 相同明文+固定密钥=相同密文，**支持去重**，安全性较低

**API支持**:
*   `POST /files/upload-encrypted` - 上传客户端加密文件
*   `GET /files/{fileId}/encryption` - 获取加密元数据
*   `POST /files/convergent-check` - 检查收敛加密文件秒传

---

## 🛠️ 项目结构

```text
frontend/
├── css/
│   └── main.css          # 全局样式（响应式布局）
├── js/
│   ├── app.js            # 入口文件
│   ├── api.js            # 后端 API 封装（HTTP请求）
│   ├── auth.js           # 认证逻辑
│   ├── config.js         # 全局配置（API地址等）
│   ├── fileManager.js    # 文件列表/操作逻辑
│   ├── uploadManager.js  # 上传/秒传/断点续传逻辑
│   ├── syncManager.js    # SSE 实时同步逻辑
│   ├── hashCalculator.js # 文件哈希计算工具（Web Crypto API）
│   ├── deltaSyncManager.js # 差分同步（增量同步）逻辑
│   └── encryptionManager.js # 客户端加密管理（Web Crypto API）
├── index.html            # 单页应用入口
└── README.md             # 本文档
```

## 🔗 后端API端点

前端通过以下API与后端交互：

| API端点 | 功能 | 调用位置 |
|---------|------|---------|
| `POST /files/quick-check` | 检查是否可以秒传 | `api.checkQuickUpload()` |
| `POST /files/quick-upload` | 执行秒传 | `api.quickUpload()` |
| `POST /files/upload` | 普通文件上传 | `api.uploadFile()` |
| `POST /files/resumable/init` | 初始化断点续传 | `api.initResumableUpload()` |
| `POST /files/resumable/{id}/chunk/{index}` | 上传分块 | `api.uploadChunk()` |
| `POST /files/resumable/{id}/complete` | 完成断点续传 | `api.completeResumableUpload()` |
| `GET /files/{id}/download` | 下载文件 | `api.downloadFile()` |
| `GET /files/{id}/signatures` | 获取文件块签名（差分同步） | `api.getFileSignatures()` |
| `POST /files/{id}/delta` | 应用差分更新 | `api.applyDelta()` |
| `POST /files/upload-encrypted` | 上传加密文件 | `api.uploadEncryptedFile()` |
| `GET /files/{id}/encryption` | 获取加密元数据 | `api.getEncryptionMetadata()` |
| `POST /files/convergent-check` | 检查收敛加密秒传 | `api.checkConvergentQuickUpload()` |
